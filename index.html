<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0023)https://jonbarron.info/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="“width=800”">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    .blinking{
    animation:blinkingText 0.8s infinite;
   }
     @keyframes blinkingText{
     0%{     color: #FF0000;    }
     49%{    color: transparent; }
     50%{    color: transparent; }
     99%{    color:transparent;  }
     100%{   color: #FF0000;    }
  }

    span.highlight {
        background-color: #ffffd0;
    }
  </style>

  <title>Anand Mishra</title>

  <link href="./css" rel="stylesheet" type="text/css">
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Anand Mishra, PhD </name><br>
          CSE-210, Department of Computer Science and Engineering<br>
          Indian Institute of Technology Jodhpur<br>
          Jodhpur - 342037 (RJ), India
        </p>
        <p>I am an Assistant Professor at the <a href="https://cse.iitj.ac.in/">Department of Computer Science and Engineering</a> of the <a href="http://iitj.ac.in">Indian Institute of Technology Jodhpur</a>. Previously,
        I worked with <a href="https://parthatalukdar.github.io/">Dr. Partha Pratim Talukdar</a> and <a href="http://visual-computing.in/wp-content/uploads/2017/08/anirban-chakraborty.html">Dr. Anirban Chakraborty</a> at the <a href="https://iisc.ac.in/"> Indian Institute of Science </a> for nearly
                two years on Knowledge-aware Computer Vision. I did my PhD working under supervision of <a href="https://faculty.iiit.ac.in/~jawahar/">Prof. C. V. Jawahar</a> and <a href="https://lear.inrialpes.fr/people/alahari/">Dr. Karteek Alahari</a> on <a href="./files/mishraThesis.pdf">understanding text in scene images</a> at <a href="https://www.iiit.ac.in/">IIIT Hyderabad</a>. At IIIT, I was a recipient of Microsoft Research India PhD fellowship 2012 and <a href="./files/mishra-xrci.pdf">XRCI best doctoral dissertation award: first runner up 2015</a>. </p>

<p>My current research interest spans Computer Vision, Language and Knowledge Graphs. To be specific, I focus on developing AI models that have the ability to acquire world and commonsense knowledge and use that knowledge to reason about the visual world and address fundamental vision tasks.
</p>


                           </div>

        <p align=center>
          <a href="mailto:mishra@iitj.ac.in">Email</a>&nbsp;|
          <a href="./files/mishraCV_Dec2020.pdf">CV</a>&nbsp;|
          <a href="https://scholar.google.co.in/citations?user=vhUg2zIAAAAJ&hl=en">Google Scholar</a>&nbsp;|
          <a href="https://dblp.org/pers/hd/m/Mishra_0001:Anand">DBLP</a></a>&nbsp;|
        <a href="#pubs">Selected Publications</a>&nbsp;|
          <a href="#teaching">Teaching</a>&nbsp;|
          <a href="https://vl2g.github.io/">VL2G</a>
          </p>

              <p align=left>

          </p>
         <br><p align=left>
              <h3>Recent/upcoming professional activities:</h3>
              <li> <b>Reviewer and/or PC member for:</b> ICLR'23, ECCV'20/22, CVPR'20/22/23, AAAI'20/21/22, ICCV'19/21/23, IJCAI'19/20, ICDAR'19, IEEE TPAMI, IJCV, IEEE TKDD, CVIU, IJDAR, Pattern Recognition.</li>
      <li> <b>Co-organizer:</b> Workshop on <a href="http://cvit.iiit.ac.in/scaldoc2023/">Scaling-up Document Image Understanding</a> (in conjunction with ICDAR'23), </b> <a href="http://icfhr2022.org/call-for-wt.php">ICFHR 2022</a> (Workshop Co-Chair), <a href="https://wdar21.github.io/">5th Workshop on Document Analysis and Recognition</a>
  in conjunction with <a href="https://iitj.ac.in/icvgip2021/">ICVGIP 2021</a>, Workshop on Knwoledge Bases and Multiple Modalities (KBMM) under AKBC 2019/2020.
              </p>
        </td>
        <td width="22%">
        <img width=100% src="./files/mishra_oct15.jpg">
        </td>
      </tr>
      </tbody>
  
  </table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            
      <tbody><tr>
        <td width="67%" valign="middle">
                <p>
                  <heading>News/Activities (A complete list is <a href="old_news.html">here</a>)</heading>
                <ul>
                   <li><b>[May 2023]</b> Recognized as one of the Outstanding Reviewers at CVPR 2023. See the <a href="https://cvpr2023.thecvf.com/Conferences/2023/OutstandingReviewers">list</a>. </li>
                  <li>
                    <b>[April 2023]</b> Our work on <a href="https://vl2g.github.io/projects/retvqa/">retVQA</a> and <a href="https://vl2g.github.io/projects/floco/">Floco-T5</a> have been accepted in <a href="https://ijcai-23.org/">IJCAI 2023 (Main Track)</a> and <a href="https://icdar2023.org/">ICDAR 2023</a>, respectively. 
                  <li>
                    <b>[March 2023]</b> Our work on Few-shot Referring Relationships in Videos got accepted in <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>.  </li> 
                  <li>
                    <b>[January 2023]</b> I am in the organizing team of <a href="https://events.iitj.ac.in/ncvpripg2023/">NCVPRIPG'23</a>. Please consider participating.
                  </li> 
                  <li>
                    <b>[January 2023]</b> Speaking at <a href="https://event.india.acm.org/ARCS/">ACM-India ARCS'23</a>.
                  </li> 
                  <li>
                     <b>[October 2022]</b> Thanks to Accenture Labs for a Gift Grant.
                  </li> 
                   <li>
                     <b>[October 2022]</b> Our works <a href="https://vl2g.github.io/projects/cofar/">COFAR</a>, <a href="https://vl2g.github.io/projects/vistot/">VisTOT</a> and <a href="https://iiscaditaytripathi.github.io/sgl/">Scene Graph Grounding</a> has been accpeted at <a href="https://www.aacl2022.org/paper">AACL-IJCNLP 2022</a>, <a href="https://2022.emnlp.org/">EMNLP 2022</a> and <a href="https://wacv2023.thecvf.com/home">WACV 2023</a>, respectively. 
                  </li>                        
                  <li>
<b>[March 2022]</b> Speaking at Search Technology Centre India (STCI), Microsoft. 
                  </li> 
                  
      </tr>
      </tbody>
</table>

      <div id="pubs">
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="100%" valign="middle">
        <heading>Selected Publications </heading> <br>
                <h3>Journals</h3>
                <ul>
                <li>
                Multimodal Query-guided Object Localization,<br />
                Aditay Tripathi, Rajath R. Dani, <b>Anand Mishra</b>, Anirban Chakraborty<br/>
                  <i> <b> Multimedia Tools and Applications, 2023</b> (Accepted)
                </li>
                <br/>           
                  
                <li>
                DHFML: deep heterogeneous feature metric learning for matching photograph and cartoon pairs <br/>
                <b>Anand Mishra</b><br/>
                pages: 1-8, <i>International Journal of Multimedia Information Retrieval 2018 <br />
                [<a target="_blank" rel="nofollow" href="https://link.springer.com/article/10.1007/s13735-018-0160-4?wt_mc=Internal.Event.1.SEM.ArticleAuthorOnlineFirst&utm_source=ArticleAuthorOnlineFirst&utm_medium=email&utm_content=AA_en_06082018&ArticleAuthorOnlineFirst_20181119#citeas">Link</a>][<a href='./files/ijmir19.bib'>bibtex</a>]</i>
                </li><br/>


                <li>
                Unsupervised refinement of color and stroke features for text binarization<br />
                <b>Anand Mishra</b>, Karteek Alahari and C. V. Jawahar<br/>
                Volume 20:105–121, <i>International Journal on Document Analysis and Recognition 2017 <br />
                [<a target="_blank" rel="nofollow" href="https://researchweb.iiit.ac.in/~anand.mishra/Home_files/mishraIJDAR17.pdf">PDF</a>][<a href='./files/ijdar17.bib'>bibtex</a>]</i>
 </li><br/>


                <li>
                Enhancing Energy Minimization Framework for Scene Text Recognition with Top-Down Cues<br />
                <b>Anand Mishra</b>, Karteek Alahari and C. V. Jawahar<br/>
                Volume 145: 30-42, <i>Computer Vision and Image Understanding 2016 <br />
                [<a target="_blank" rel="nofollow" href="https://researchweb.iiit.ac.in/~anand.mishra/Home_files/mishraCVIU16.pdf">PDF</a>][<a href='./files/cviu16.bib'>bibtex</a>]</i>
                </li>


                </ul>
                <h3>Conference Papers</h3>
             <li> Answer Mining from a Pool of Images: Towards Retrieval-Based Visual Question Answering, <span class="blinking">(NEW)</span>  <br />
            Abhirama Subramanyam Penamakuri, Manish Gupta, Mithun Das Gupta, <b>Anand Mishra</b>
          <br/>
                <i> <b>IJCAI 2023. </b> <br />
          </li>
           
                <br/>  
           <li> Towards Making Flowchart Images Machine Interpretable <span class="blinking">(NEW)</span>  <br />
           Shreya Shukla, Prajwal Gatti, Yogesh Kumar, Vikash Yadav, <b>Anand Mishra</b>
          <br/>
                <i> <b>ICDAR 2023. </b> <br />
          </li>
          
                <br/>     
                 
               
                  
           <li> Few-Shot Referring Relationships in Videos, 
              <span class="blinking">(NEW)</span>  <br />
            Yogesh Kumar, <b>Anand Mishra</b>
          <br/>
                <i> <b>CVPR 2023. </b> <br />
                  </li>
            <br/> 
          
           <li> Grounding Scene Graphs on Natural Images via Visio-Lingual Message Passing
              <span class="blinking">(NEW)</span>  <br />
            Aditay Tripathi, <b>Anand Mishra</b>, Anirban Chakraborty,
<br/>
                <i> <b>WACV 2023. </b> <br />
                [<a target="_blank" rel="nofollow" href="https://arxiv.org/pdf/2211.01969.pdf">Paper</a>][<a target="_blank" rel="nofollow" href="https://iiscaditaytripathi.github.io/sgl/">Project Page</a>][<a target="_blank" rel="nofollow" href="https://github.com/IISCAditayTripathi/Scene-graph-localization">Code</a>] 
        </li>
                <br/> 
            
          
          <li> VISTOT: Vision-Augmented Table-to-Text Generation,
              <span class="blinking">(NEW)</span>  <br />
            Prajwal Gatti, <b>Anand Mishra</b>, Manish Gupta, Mithun Das Gupta,<br/>
                <i> <b>EMNLP 2022. </b> <br />
                [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/vistot/docs/VISTOT-EMNLP2022.pdf">Paper</a>][<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/vistot/">Project Page</a>][<a target="_blank" rel="nofollow" href="">Code</a>] 
        </li>
                <br/>
            
      
          
          
          <li> COFAR: Commonsense and Factual Reasoning in Image Search
              <span class="blinking">(NEW)</span>  <br />
            Prajwal Gatti, Abhirama Subramanyam Penamakuri, Revant Teotia, <b>Anand Mishra</b>, Shubhashis Sengupta, Roshni Ramnani<br/>
                <i> <b>AACL-IJCNLP 2022. </b> <br />
                [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/cofar/docs/COFAR-AACL2022.pdf">Paper</a>][<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/cofar/">Project Page</a>][<a target="_blank" rel="nofollow" href="https://github.com/vl2g/cofar">Code</a>] 
        </li>
                <br/>
          
          
          
          <li> Few-shot Visual Relationship Co-localization 
                <br />
                 Revant Teotia*, Vaibhav Mishra*, Mayank Maheshwari*, <b>Anand Mishra</b>,<br/>
                <i> <b>ICCV 2021. </b> <br />
                [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/vrc/docs/VRC-ICCV2021.pdf">Paper</a>][<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/vrc/">Project Page</a>][<a target="_blank" rel="nofollow" href="https://github.com/vl2g/VRC.git">Code</a>] 
                  (*: equal contribution)
                  </li>
                <br/>
          
                <li> Look, Read and Ask: Learning to Ask Questions by Reading Text in Images
                ,<br />
                 Soumya Jahagirdar, Shankar Gangisetty, <b>Anand Mishra</b>,<br/>
                <i> <b>ICDAR 2021 (Oral). </b> <br />
                [<a target="_blank" rel="nofollow" href="./files/JahagirdarICDAR2021.pdf">Paper</a>]           
                  </li>
                <br/>
          
                <li>
                Sketch-Guided Object Localization in Natural Images,<br />
                Aditay Tripathi, Rajath R. Dani, <b>Anand Mishra</b>, Anirban Chakraborty<br/>
                <i> <b>ECCV 2020 (Spotlight Presentation)</b>. <br />
                [<a target="_blank" rel="nofollow" href="./files/TripathiECCV2020.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/ECCV2020.bib'>bibtex</a>]
                [<a href="http://visual-computing.in/sketch-guided-object-localization/">Project page</a>][<a href="https://github.com/IISCAditayTripathi/SketchGuidedLocalization">Code</a>]
                [<a href="https://www.youtube.com/watch?v=vI0NEVytLfU">Know the paper in 90 seconds</a>] [<a href="https://www.youtube.com/watch?v=5gBDPbbvssk">Know the paper in ten minutes</a>]
                </li>
                <br/>

                <li>
                From Strings to Things: Knowledge-enabled VQA model that can read and reason,<br />
                Ajeet Kumar Singh, <b>Anand Mishra</b>, Shashank Shekhar, and Anirban Chakraborty<br/>
                <i> <b>ICCV 2019 (oral)</b>. <br />
                [<a target="_blank" rel="nofollow" href="https://textkvqa.github.io/docs/textKVQA_ICCV2019.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/ICCV2019.bib'>bibtex</a>]
                [<a href="https://textkvqa.github.io">Project page</a>]
                </li>
                <br/>

                <li>
                OCR-VQA: Visual Question Answering by Reading Text in Images <br />
                <b>Anand Mishra</b>, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty<br/>
                <i> <b>ICDAR 2019</b>. <br />
                [<a target="_blank" rel="nofollow" href="./files/mishra-OCR-VQA.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/ICDAR2019.bib'>bibtex</a>]
                [<a href="https://ocr-vqa.github.io">Project page</a>]
                </li><br/>

                <li>
                KVQA: Knowledge-aware Visual Question Answering <br />
                Sanket Shah*, <b>Anand Mishra*</b>, Naganand Yadati and Partha Pratim Talukdar<br/> (*: equal contribution)
                <i> <b>AAAI 2019</b>. (acceptance rate: 16.1%) <br />
                [<a target="_blank" rel="nofollow" href="http://dosa.cds.iisc.ac.in/kvqa/KVQA-AAAI2019.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/aaai19.bib'>bibtex</a>]
                [<a href="http://malllabiisc.github.io/resources/kvqa/">Project page</a>]
                </li><br/>

                <li>
                Deep Embedding using Bayesian Risk Minimization with Application to Sketch Recognition <br />
                <b>Anand Mishra</b>,and Ajeet Kumar Singh<br/>
                <i> <b>ACCV, 2018</b>. (acceptance rate: 28%) <br />
                [<a target="_blank" rel="nofollow" href="https://arxiv.org/abs/1812.02466">Paper (arXiv)</a>] [<a target="_blank" rel="nofollow"href='./files/accv18.bib'>bibtex</a>]
                </li><br/>

                <li>
                IIIT-CFW: A Benchmark database of Cartoon Faces in the Wild<br />
                Ashutosh Mishra, Shyam N. Roy, <b>Anand Mishra</b>,and C. V. Jawahar<br/>
 <i> <b>ECCVW, 2016</b>. (Oral) <br />
                [<a target="_blank" rel="nofollow" href="http://cvit.iiit.ac.in/images/ConferencePapers/2016/Mishra-ECCVW2016.pdf">PDF</a>] [<a target="_blank" rel="nofollow" href='./files/eccvw18.bib'>bibtex</a>][<a href="https://cvit.iiit.ac.in/research/projects/cvit-projects/cartoonfaces"> IIIT-CFW dataset</a>]
                </li><br/>

                <li>
                A Simple and Effective method for Script Identification in the Wild<br />
                Ajeet Kumar Singh, <b>Anand Mishra</b>, Pranav Dabaral and C. V. Jawahar<br/>
                <i> <b>DAS, 2016</b>. <br />
                [<a target="_blank" rel="nofollow" href="https://researchweb.iiit.ac.in/~anand.mishra/Home_files/singhDAS2016.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/das16.bib'>bibtex</a>]
                </li><br/>

                <li>
                Scene Text Recognition and Retrieval for Large Lexicons<br />
                Udit Roy, <b>Anand Mishra</b>, Karteek Alhari and C. V. Jawahar<br/>
                <i> <b>ACCV 2014</b>. <br />
                [<a target="_blank" rel="nofollow" href="https://hal.inria.fr/hal-01088739/document">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/accv14.bib'>bibtex</a>]
                </li><br/>

                <li>
                Image Retrieval using Textual Cues<br />
                <b>Anand Mishra</b>, Karteek Alhari and C. V. Jawahar<br/>
                <i> <b>ICCV, 2013</b>. <br />
                [<a target="_blank" rel="nofollow" href="./files/mishraICCV13.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/iccv13.bib'>bibtex</a>]
                </li><br/>

                <li>
                Whole is Greater than Sum of Parts: Recognizing Scene Text Words<br />
                Vibhor Goel, <b>Anand Mishra</b>, Karteek Alhari and C. V. Jawahar<br/>
                <i> <b>ICDAR, 2013</b>. <br />
                [<a target="_blank" rel="nofollow" href="https://researchweb.iiit.ac.in/~anand.mishra/Home_files/goelICDAR13.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/icdar13_g.bib'>bibtex</a>]
                </li><br/>

                <li>
                Scene Text Recognition using Higher Order Language Priors<br />
                <b>Anand Mishra</b>, Karteek Alhari and C. V. Jawahar<br/>
                <i> <b>BMVC 2012</b>. (Oral) <br />
                [<a target="_blank" rel="nofollow" href="https://researchweb.iiit.ac.in/~anand.mishra/Home_files/mishraBMVC12.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/bmvc12.bib'>bibtex</a>]
                        [<a href="http://cvit.iiit.ac.in/projects/SceneTextUnderstanding/IIIT5K.html"> IIIT-5K Word dataset</a>]
                </li><br/>


                <li>
                Top-down and Bottom-up cues for Scene Text Recognition<br />
                <b>Anand Mishra</b>, Karteek Alhari and C. V. Jawahar<br/>
                <i> <b>CVPR 2012</b>. <br />
                [<a target="_blank" rel="nofollow" href="https://researchweb.iiit.ac.in/~anand.mishra/Home_files/mishraCVPR12.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/cvpr12.bib'>bibtex</a>]
                </li><br/>

                <li>
                An MRF model for Binarization of Natural Scene Text<br />
                <b>Anand Mishra</b>, Karteek Alhari and C. V. Jawahar<br/>
  <i> <b>ICDAR 2011</b>. (Oral) <br />
                [<a target="_blank" rel="nofollow" href="https://researchweb.iiit.ac.in/~anand.mishra/Home_files/mishraICDAR11.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/bmvc11.bib'>bibtex</a>]
                </li>

                <br/>
                </ul>
        </td>
      </tr>
      </tbody></table>
</div>


<div id="teaching">
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="100%" valign="middle">
        <heading>Teaching</heading> <br>
          <h3> At IIT Jodhpur </h3>
          <li> CSL7360: Computer Vision (Spring’23)</li>
          <li> CSL2040: Maths for Computing (Monsoon’22/21/AY 20-21–Tri-3) </li>
          <li> CSL7410: Graph Theory and Application (AY 20-21–Tri-1, Spring’22)</li>
          <li> CS222: Theory of Computation (Spring’20)</li>
          <li> CS212: Object-oriented Design and Analysis (Monsoon’19)</li>
 
           <h3> At IISc Bangalore</h3>
          <li> UE101: Algorithms and Programmin (Monsoon'18)</li>
              - Co-taught at Indian Institute of Science with Dr. Sathish Govindrajan and Dr. Viraj Kumar. 
                
            <h3> At IIIT Hyderabad (during PhD)</h3>
          <li> Computer Problem Solving (Monsoon'16) </li>
               -- Introductory course for M.Tech. Bioinformatics (class strength: 48) 
          
          <h3> At IIIT Sri City (during PhD)</h3>
          <li> Computer Architecture (Spring'15)</li>
           -- Co-taught at IIIT Sricity as visiting instructor with Dr. Suresh Purini and Prof. Govindrajulu</li>
                
        <h3> Operating Systems (Monsoon'14) </h3>
        -- Co-taught at IIIT Sricity as visiting instructor with Dr. Suresh Purini </li>
                

    </td>
      </tr>
      </tbody></table>
</div>
