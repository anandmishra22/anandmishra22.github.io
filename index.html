<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0023)https://jonbarron.info/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="“width=800”">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    .blinking{
    animation:blinkingText 0.8s infinite;
   }
     @keyframes blinkingText{
     0%{     color: #FF0000;    }
     49%{    color: transparent; }
     50%{    color: transparent; }
     99%{    color:transparent;  }
     100%{   color: #FF0000;    }
  }

    span.highlight {
        background-color: #ffffd0;
    }
    .sticky-note {
            background-color: #ffffcc;
            padding: 10px;
            border: 1px solid #cccc00;
            width: fit-content;
            position: sticky;
            top: 20px; /* Adjust top position as needed */
            z-index: 1000;
        }
  </style>

  <title>Anand Mishra</title>

  <link href="./css" rel="stylesheet" type="text/css">
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Anand Mishra, PhD </name><br>
          CSE-210, Department of Computer Science and Engineering<br>
          Indian Institute of Technology Jodhpur<br>
          Jodhpur - 342030 (RJ), India
        </p>
        <p align="justify"> Currently, I serve as an Assistant Professor at the <a href="https://cse.iitj.ac.in/">Department of Computer Science and Engineering</a> at the <a href="http://iitj.ac.in">Indian Institute of Technology Jodhpur</a>. Prior to this role, I had the opportunity to work as a Postdoctoral Researcher under the mentorship 
          of <a href="https://parthatalukdar.github.io/">Dr. Partha Pratim Talukdar</a> at the <a href="https://iisc.ac.in/">Indian Institute of Science</a>, focusing on Knowledge-aware Computer Vision for nearly two years. 
          For my doctoral studies, I conducted research on <a href="./files/mishraThesis.pdf">the interpretation of text within scene images</a> at <a href="https://www.iiit.ac.in/">IIIT Hyderabad</a>, 
          where I had the privilege of being supervised by <a href="https://faculty.iiit.ac.in/~jawahar/">Prof. C. V. Jawahar</a> and <a href="https://lear.inrialpes.fr/people/alahari/">Dr. Karteek Alahari</a>.
          </p>
       <p align="justify">   
       My current research interest lies in the intersection of vision and language. 
       Specifically, I am deeply engaged in exploring the field of developing AI Agents 
       that can understand human language and perceive and comprehend the visual world.
       The overarching goal of my research group at IIT Jodhpur, known as Vision, Language, and Learning Group or VL2G in short, is 
       to advance the development of these intelligent agents towards bridging the gap between human and machine interaction.
       To know more about the recent research focus and activities of VL2G, please visit the group's <a href="https://vl2g.github.io/">website</a>.  

          
          </p>


                           </div>

        <p align=center>
          <a href="mailto:mishra@iitj.ac.in">Email</a>&nbsp;|
          <a href="./files/mishraCV_April2025.pdf">CV</a>&nbsp;|
          <a href="https://scholar.google.co.in/citations?user=vhUg2zIAAAAJ&hl=en">Google Scholar</a>&nbsp;|
          <a href="https://dblp.org/pers/hd/m/Mishra_0001:Anand">DBLP</a></a>&nbsp;|
        <a href="#pubs">Selected Publications</a>&nbsp;|
          <a href="#teaching">Teaching</a>&nbsp;|
          <a href="https://vl2g.github.io/">VL2G</a>
          </p>

              <p align=left>

          </p>

        
         <br><p align=left>
              <h3>Recent/upcoming professional activities:</h3>
           <li> <b>Area Chair:</b> <a href="https://cvpr.thecvf.com/Conferences/2026">CVPR'26</a>.
        <li> <b>Program Co-Chair:</b> <a href="https://icvgip.in/2025/">ICVGIP'25</a>.
              <li> <b>Reviewer and/or PC member for: </b> CVPR'20/22/23/24/25, ECCV'20/22/24, ICCV'19/21/23, ACL Rolling Review, ICLR'23, WACV'23, AAAI'20/21/22, IJCAI'19/20, ICDAR'19, IEEE TPAMI, IJCV, IEEE TKDD, CVIU, IJDAR, Pattern Recognition.</li>
      <li> <b>Co-organizer:</b> <a href="http://cvit.iiit.ac.in/scaldoc2023/">ScalDoc 2023</a>, 
        <a href="https://events.iitj.ac.in/ncvpripg2023/">NCVPRIPG'23</a>, 
        <a href="https://wdar21.github.io/">WDAR 2021</a>/<a href="https://ilocr.iiit.ac.in/wdar2023/">WDAR 2023</a>, 
        <a href="https://kb-mm.github.io/">KBMM 2019</a>/<a href="https://kb-mm-2020.github.io/">KBMM 2020</a>.
        <li> <b>Workshop Co-Chair:</b> <a href="https://icfhr2022.org/">ICFHR'22</a>.
                      </p>
        </td>
        <td width="22%">
        <img width=100% src="./files/Mishra_oct22.png">
        </td>
      </tr>
      </tbody>
  </table>
    
 

    
  <div style="height: 500px; overflow-y: auto;">
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
            <tr>
                <td width="67%" valign="middle">
                    <p>
                        <heading>News/Activities</heading>
                    <ul>

                      <li>
                            <b>[November 2025]</b> Our debut work on healthcare AI -- <i>PatientVLM meets DocVLM</i> and a work on Few-shot Video Object Detection are accepted at <a href="https://aaai.org/conference/aaai/aaai-26/">AAAI 2026</a> Main Track.           
                      </li>
                      
                         <li>
                            <b>[November 2025]</b> Our works <a href="https://github.com/vl2g/MATR.git">MATR (ICCV 2025)</a> and <a href="https://github.com/vl2g/MPA.git">MPA (EMNLP 2025)</a> are selected for showcase at <a href="https://icvgip.in/2025/#visionindiaevent">Vision India</a> Session.   
                      
                      </li>

                      
                      <li>
                            <b>[August 2025]</b> Our work <a href="https://github.com/vl2g/MPA.git">Model Parity Alignment</a> towards empowering small VLMs is accepted in EMNLP 2025 (Main Track). 
                      
                      </li>
                      
                      <li>
                            <b>[July 2025]</b> Our work on <a href="https://github.com/vl2g/MATR.git">Moment Alignment Transformer (MATR)</a> is accepted in ICCV 2025. 
                      
                      </li>
                      
                      <li>
                            <b>[July 2025]</b> Speaking at CVIT Summer School at IIIT Hyderabad.
                      
                      </li>
                      <li>
                            <b>[April 2025]</b> Received Google's Gemma Academic Program Award to support our research with cloud resources.
                      
                      </li>
                       <li>
                            <b>[March 2025]</b> I am a program co-chair for <a href="https://icvgip.in/2025/">ICVGIP 2025</a>.
                      
                      </li>
                      <li>
                            <b>[December 2024]</b> Our work <a href="https://vl2g.github.io/projects/PatentLMM/">PatentLMM</a> is accepted at AAAI 2025.</li>
                      
                      <li>
                            <b>[December 2024]</b> My upcoming course, <a href="https://sites.google.com/iitj.ac.in/prmliitj/home">CSL2050: Pattern Recognition and Machine Learning</a>, is supported by Google Cloud Teaching Credits.                      
                      </li>
          
                      <li>
                            <b>[October 2024]</b> Shreya and Nakul from <a href="https://vl2g.github.io/">VL2G</a> received (in absentia) the Director's Prize for graduating student with Best Academic Innovation Work among students of all B. Tech. Programs of the class of 2024 at IITJ convocation. <a href="https://www.youtube.com/watch?v=gE1kf3TEIys&t=27508s">See the announcement</a>. <a href="https://www.linkedin.com/posts/iitjodhpur_iitjodhpur-topgraduates2024-innovationintechnology-activity-7255938185040404480-hLhu?utm_source=share&utm_medium=member_desktop">View Institute Social Media Post</a>.                      
                      </li>
                     
                        <li>
                            <b>[September 2024]</b> Our latest work on <a href="https://arxiv.org/pdf/2308.03024">scene text-to-scene text translation</a> is now available on our <a href="https://vl2g.github.io/projects/visTrans/">project website</a>. 
                         We have also made the code and data publicly available for those interested in exploring or building upon this work.
                        </li>
                

                      <li>
                            <b>[September 2024]</b> Large Multimodal Model-Extension to our work <a href="https://textkvqa.github.io/">Text-KVQA</a> (Singh et al., ICCV 2019) is accepted at Main Track of <a href="https://2024.emnlp.org/">EMNLP 2024</a>.  </li>
                      
                        <li>
                            <b>[July 2024]</b> Speaking at <a href="https://lnmiit.ac.in/department/cse/acm-rocs-2024/">ACM-India ROCS 2024</a>.
                        </li>
                        <li>
                            <b>[April 2024]</b> Work on <a href="https://arxiv.org/pdf/2404.11949.pdf">sketch-guided image inpainting</a> by <b>undergrad student</b> Nakul Sharma is accepted at <a href="https://cvlai.net/ntire/2024/">CVPR 2024 Workshop</a>.
                        </li>
                        <li>
                            <b>[December 2023]</b> Our works - <a target="_blank" rel="nofollow" href="./files/Kumar-AAAI24.pdf">QDETRv (query-guided DETR for Video)</a> and <a target="_blank" rel="nofollow" href="./files/Gatti-AAAI24.pdf">CSTBIR (composite sketch+text-based image retrieval)</a> are accepted at <a href="https://aaai.org/aaai-conference/">AAAI 2024</a>.
                        </li>
                        <li>
                            <b>[October 2023]</b> Two of our works, one in the domain of AI for Education and one in query-guided attention in vision transformers, are accepted at <a href="https://wacv2024.thecvf.com/">WACV'24</a>.
                        </li>
                        <li>
                            <b>[October 2023]</b> Speaking at Distinguished Researcher Speaker Series (DRSS), Accenture Labs (Virtually) on our video works.
                        </li>
                        <li>
                            <b>[September 2023]</b> Speaking at NISER, Bhubaneswar (virtually) on our sketch-guided visual understanding works.
                        </li>
                        <li>
                            <b>[July 2023]</b> Received the Microsoft Academic Partnership Grant (MAPG) 2023 (see the <a href="https://www.linkedin.com/posts/manishsgupta_microsoft-india-datascience-activity-7081315392941412352-9fhY">announcement</a>).
                        </li>
                        <li>
                            <b>[May 2023]</b> Recognized as one of the Outstanding Reviewers at CVPR 2023. The complete list is <a href="https://cvpr2023.thecvf.com/Conferences/2023/OutstandingReviewers">here</a>.
                        </li>
                        <li>
                            <b>[April 2023]</b> Our work on <a href="https://vl2g.github.io/projects/retvqa/">retVQA</a> and <a href="https://vl2g.github.io/projects/floco/">Floco-T5</a> are accepted in <a href="https://ijcai-23.org/">IJCAI 2023 (Main Track)</a> and <a href="https://icdar2023.org/">ICDAR 2023</a>, respectively.
                        <li>
                            <b>[March 2023]</b> Our work on Few-shot Referring Relationships in Videos is accepted in <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>.
                        </li>
                        <li>
                            <b>[January 2023]</b> I am in the organizing team of <a href="https://events.iitj.ac.in/ncvpripg2023/">NCVPRIPG'23</a>. Please consider participating.
                        </li>
                        <li>
                            <b>[January 2023]</b> Speaking at <a href="https://event.india.acm.org/ARCS/">ACM-India ARCS'23</a>.
                        </li>
                        <li>
                            <b>[October 2022]</b> Thanks to Accenture Labs for a Gift Grant.
                        </li>
                        <li>
                            <b>[October 2022]</b> Our works <a href="https://vl2g.github.io/projects/cofar/">COFAR</a>, <a href="https://vl2g.github.io/projects/vistot/">VisTOT</a> and <a href="https://iiscaditaytripathi.github.io/sgl/">Scene Graph Grounding</a> are accpeted at <a href="https://www.aacl2022.org/paper">AACL-IJCNLP 2022</a>, <a href="https://2022.emnlp.org/">EMNLP 2022</a> and <a href="https://wacv2023.thecvf.com/home">WACV 2023</a>, respectively.
                        </li>
                        <li>
                            <b>[March 2022]</b> Speaking at Search Technology Centre India (STCI), Microsoft.
                        </li>
                         <li>
<b>[March 2022]</b> Received IIT J-Research Initiation Seed Grant.
</li>
<li>
<b>[March 2022]</b> Speaking at my undergraduate alma mater on Fundamentals of Neural Networks, <a href="https://www.ggu.ac.in/">Guru Ghasidas Central University Bilaspur</a>.
</li>
<li>
<b>[February 2022]</b> Speaking at <a href="https://icmi.acm.org/2022/pcw/">ICMI Pre-Conference Workshop 2022</a>, IIIT Bangalore (Virtual).
</li>
<li>
 <b>[February 2022]</b> Speaking at <a href="http://www.wadla.in/#speakers">WADLA 2022</a>, IIIT Sri City (Virtual).
</li>
<li>
<b>[January 2022]</b> Received the SERB-Startup Research Grant.
</li>  
<li>
<b>[December 2021]</b> Two of the graduating members of my research group, <a href="https://www.linkedin.com/in/vaibhav-mishra-iitj/">Vaibhav Mishra</a> and <a href="https://www.linkedin.com/in/maheshwarimayank333/">Mayank Maheshwari</a> got the Director's Prize for the best academic innovation work among all the graduating students of 2021 batch at IIT Jodhpur convocation. (<a href="https://www.youtube.com/watch?v=J5BSK7DtPuI&t=6948s">See the award receiving moment</a>).
</li>
<li>
<b>[November 2021]</b> My PhD Student <a href="https://abhiram4572.github.io/">Abhirama P.</a> got selected as Prime Minister's Research Fellow.
</li>            
<li>
<b>[September 2021]</b> Speaking in a panel at <a href="https://sites.google.com/view/docvqaworkshop2021/program?authuser=0">DocVQA workshop</a> under ICDAR 2021.
</li>
<li>
<b>[September 2021]</b> Recognized as one of the outstanding reviewers at ICCV 2021. See <a href="http://iccv2021.thecvf.com/outstanding-reviewers">the list</a>.
</li>
<li>
<b>[July 2021]</b> Our work on Few-shot Visual Relationship Co-Localisation with Revant Teotia, Vaibhav Mishra and Mayank Maheshwari got accepted in <a href="http://iccv2021.thecvf.com/home">ICCV 2021</a>. <a href="https://vl2g.github.io/projects/vrc/">The paper and code</a> are available now.
</li>                    
<li>
<b>[June 2021]</b> Got selected for Microsoft Academic Partnership Grant (MAPG) 2021.
</li>
<li>
<b>[April 2021]</b> A paper on TextVQG got accepted in <a href="https://icdar2021.org/">ICDAR 2021</a>. Paper is available <a href="./files/JahagirdarICDAR2021.pdf">here</a>.
</li>             
<li>
<b>[February 2021]</b> Presenting a poster on 'Multimodal Machine Learning for Enhanced Image Understanding' under the 'Machine Learning and Big Data Analytics Track', at the 11th Indo German Frontier of Engineering Conference (INDOGFE 2021).
</li>                       
<li>
<b>[February 2021]</b> Speaking at <a href="https://sites.google.com/view/wadla2021">WADLA 2021 </a> (Virtual event hosted by IIIT Sri City) [<a href="https://docs.google.com/presentation/d/1kh6omLCpj2XMIDtV2847hgisOM5Z6LDpbZdHAMNHPaw/edit?usp=sharing">Slides</a>].
</li>                      
<li>
<b>[December 2020]</b> Our ICCV 2019 paper on <a href="https://textkvqa.github.io">textKVQA</a> is accepted for a presentation at <a href="http://www.iitj.ac.in/icvgip20-21/2020/visionIndia.php">Vision India</a>.
</li>
<li>
<b>[August 2020]</b> Received <b>IIT-Jodhpur Teaching Excellence Award 2020</b> (see <a href="http://www.youtube.com/watch?v=X16iuc4neic&t=14m10s">announcement</a>). </li>
<li>
<b>[July 2020]</b> One paper got accepted at <a href="https://eccv2020.eu">ECCV 2020</a> as spotlight (top-5% paper). This was joint work with Aditay, Rajath and Anirban. </li>
<li>
<b>[June 2020]</b> Speaking as an invited speaker at <a href="https://ece.iisc.ac.in/~spcom/2020/ss_dl_cv.html">Deep Learning for Computer Vision Session at SPCOM 2020 </a> going to be organized virtually at IISc Bangalore.  
</li>
<li>
<b>[June 2020]</b> Got a research grant from the Accenture labs.  
</li>
<li>
<b>[April 2020]</b> Co-organizing 2nd workshop on <a href ="https://kb-mm-2020.github.io"> KBMM</a> co-located with <a href="https://www.akbc.ws/2020/">AKBC 2020</a>.  
</li>
<li>
<b>[December 2019]</b> Gave a talk on our recent works on knowledge-aware Computer Vision to a small group of developers/researchers from Siemens at IISc Bangalore.
</li>
<li>
<b>[July 2019]</b> Our paper on the "Knowledge-enabled" VQA model that can read got accepted in <a href="http://iccv2019.thecvf.com">ICCV 2019</a> for an oral presentation.
</li>
<li>
<b>[July 2019]</b> Joined IIT-J.
</li>
<li>
<b>[June 2019]</b> Gave a talk on "Reading Text in Scene Images, Bridging it to World Knowledge, and Beyond" at <a href="http://www.iitg.ac.in/cse/home">Department of CSE, IIT Guwahati</a>.
</li>
<li>
<b>[May 2019]</b> Our OCR-VQA paper got accepted in <a href="https://icdar2019.org/">ICDAR 2019</a>. </a>
</li>
<li>
<b>[April 2019]</b> Gave a talk on "Knowledge-aware Visual Question Answering" at <a href="https://www.iiitb.ac.in/">IIIT Bangalore</a>.
</li>
<li>
<b>[April 2019]</b>: We (with Sameer Singh and Pouya Pezeshkpour from University of California, Irvin and Partha Talukdar from IISc) are organizing a workshop: "<a href="https://kb-mm.github.io/">Knowledge Bases and Multiple Modalities (KBMM)</a>" at <a href="http://www.akbc.ws/2019/">Automated Knowledge Base Construction (AKBC) 2019</a>. Please consider submitting an extended abstract related to Knowledge Bases and Multiple Modalities work.                
</li>
<li>
<b>[March 2019]</b>: I will be serving as a program committee member for <a href="https://icdar2019.org/">ICDAR 2019</a>.</li>
<li>
<b>[February 2019]</b>: I will be serving as a reviewer for <a href="http://iccv2019.thecvf.com/">ICCV 2019</a>.
</li>
<li>
<b>[February 2019]:</b> Offered a lecture on Graph Representation Learning in Deep Learning for Computer Vision course at IISc Bangalore.
</li>
<li>
<b>[January 2019]:</b> Obtained <b>research grant</b> from Siemens. Will be working as co-PI with Dr. Anirban and Dr. Partha on this project.
</li>
<li>
<b>[January 2019]</b>: I will be serving as a program committee member for <a href="https://ijcai19.org/">IJCAI 2019</a>.
</li>
<li>
<b>[December 2018]:</b> I am attending AAAI 2019. My travel is supported by a gift of USD 3000 by Google to the Indian Institute of Science. Thank you Google. </b>
</li>
<li>
<b>[November 2018]:</b> I am serving as a program committee member for <a href="http://cse.iitj.ac.in/dar2018/">Workshop on Document Analysis and Recognition (DAR) 2018</a>. (to be held as part of <a href="https://cvit.iiit.ac.in/icvgip18/index.php">ICVGIP 2018</a>)</b>
</li>
<li>
<b>[November 2018]:</b> Our work on "Knowledge-Aware Visual Question Answering" got accepted in <b> AAAI 2019 (acceptance rate = 16 %)</b>
</li>
<li>
<b>[October 2018]:</b> My solo author paper on deep heterogeneous metric learning got accepted for publication at <br>
  <a href="https://www.springer.com/computer/information+systems+and+applications/journal/13735">International Journal of Multimedia Information Retrieval (IJMIR)</a>. This paper addresses the problem of matching cartoon and real faces.
</li>
<li>
<b>[September 2018]:</b> Our paper on deep metric learning got accepted at ACCV 2018. Pre-print will be available soon!
</li>
<li>
<b>[September 2018]:</b> I will be attending <a href="https://ard.amazon-ml.com/bangalore/">Amazon Research Day</a> on September 28.
</li>
<li>
<b>[July 2018]:</b>  I will be co-teaching an undergraduate introductory course on Algorithms and Programming at IISc Bangalore. <a href="http://drona.csa.iisc.ernet.in/~gsat/Course/algoandprog/">[Link]</a>
</li> 
<li>
<b>[August 2017]:</b>  Joined IISc Bangalore as a PostDoc.
</li> 
                    </ul>
                </td>
            </tr>
        </tbody>
    </table>
</div>

      <div id="pubs">
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="100%" valign="middle">
        <heading>Selected Publications </heading> <br>
                <h3>Journals</h3>
                <ul>

                <li>
                Bharat Scene Text: A Novel Comprehensive Dataset and Benchmark for Indian Language Scene Text Understanding,<br />
                Anik De, Abhirama Subramanyam Penamakuri, Rajeev Yadav, Aditya Rathore, Harshiv Shah, Devesh Sharma, Sagar Agarwal, Pravin Kumar, <b>Anand Mishra</b><br/>  
                [<a target="_blank" rel="nofollow" href="https://www.arxiv.org/pdf/2511.23071">Preprint</a>][<a target="_blank" rel="nofollow" href="https://github.com/Bhashini-IITJ/BharatSceneTextDataset">Bharat Scene Text Dataset</a>][<img src="ipo.png" width="120"/>](https://github.com/Bhashini-IITJ/IndicPhotoOCR)
</br>
                   Under Peer Review.</i></br>
                </li>
                </br>
                <li>
                Moment Alignment Transformer for Video-to-Video Moment Retrieval,<br/>
                Yogesh Kumar, Uday Agarwal, Manish Gupta, <b>Anand Mishra</b><br/>  
                [<a target="_blank" rel="nofollow" href="https://www.techrxiv.org/users/980524/articles/1346754-moment-alignment-transformer-for-video-to-video-moment-retrieval">Preprint</a>]</br>
                   Under Peer Review.</i></br> 
                </li>
                </br>
                <li>
                Bridging language to visuals: towards natural language query-to-chart image retrieval,<br />
                Neelu Verma, Anik De, <b>Anand Mishra</b><br/>
                   Volume: 13 (3), 32 , <i>International Journal of Multimedia Information Retrieval, 2024.</i></br> 
                    [<a href=""> Link </a>]
                </li>
                <br/>  
                  
                <li>
                Multimodal Query-guided Object Localization,<br />
                Aditay Tripathi, Rajath R. Dani, <b>Anand Mishra</b>, Anirban Chakraborty<br/>
                   Volume: 83 (5), Pages: 14857-14881, <i>Multimedia Tools and Applications, 2024.</i></br> 
                    [<a href="https://rdcu.be/dpps6"> Link </a>]
                </li>
                <br/>  

               
                  
                <li>
                DHFML: deep heterogeneous feature metric learning for matching photograph and cartoon pairs <br/>
                <b>Anand Mishra</b><br/>
                pages: 1-8, <i>International Journal of Multimedia Information Retrieval, 2018.</i> <br />
                [<a target="_blank" rel="nofollow" href="https://link.springer.com/article/10.1007/s13735-018-0160-4?wt_mc=Internal.Event.1.SEM.ArticleAuthorOnlineFirst&utm_source=ArticleAuthorOnlineFirst&utm_medium=email&utm_content=AA_en_06082018&ArticleAuthorOnlineFirst_20181119#citeas">Link</a>][<a href='./files/ijmir19.bib'>bibtex</a>]</i>
                </li><br/>


                <li>
                Unsupervised refinement of color and stroke features for text binarization<br />
                <b>Anand Mishra</b>, Karteek Alahari and C. V. Jawahar<br/>
                Volume 20:105–121, <i>International Journal on Document Analysis and Recognition 2017 <br />
                [<a target="_blank" rel="nofollow" href="https://researchweb.iiit.ac.in/~anand.mishra/Home_files/mishraIJDAR17.pdf">PDF</a>][<a href='./files/ijdar17.bib'>bibtex</a>]</i>
 </li><br/>


                <li>
                Enhancing Energy Minimization Framework for Scene Text Recognition with Top-Down Cues<br />
                <b>Anand Mishra</b>, Karteek Alahari and C. V. Jawahar<br/>
                Volume 145: 30-42, <i>Computer Vision and Image Understanding 2016 <br />
                [<a target="_blank" rel="nofollow" href="https://researchweb.iiit.ac.in/~anand.mishra/Home_files/mishraCVIU16.pdf">PDF</a>][<a href='./files/cviu16.bib'>bibtex</a>]</i>
                </li>
                </ul>
          
                <h3>Conference Papers</h3>

            <li> PatientVLM Meets DocVLM: Pre-Consultation Dialogue Between Vision-Language Models for Efficient Diagnosis<br/>
           K Lokesh, Uday Agarwal, Abhirama Subramanyam Penamakuri, Apoorva Challa, Shreya K Gowda, Somesh Gupta, <b>Anand Mishra</b>
          <br/>
                <i> <b>AAAI 2026.</b><span class="blinking">(NEW)</span> <br/>
           </li>
          [<a target="_blank" rel="nofollow" href="">Paper</a>]
          [<a target="_blank" rel="nofollow" href="">Code</a>] COMING SOON
                     <br/> <br/>

           <li> Temporal Object-Aware Vision Transformer for Few-Shot Video Object Detection<br/>
           Yogesh Kumar, <b>Anand Mishra</b>
          <br/>
                <i> <b>AAAI 2026.</b><span class="blinking">(NEW)</span> <br/>
           </li>
          [<a target="_blank" rel="nofollow" href="">Paper</a>]
          COMING SOON
                     <br/> <br/>

           <li> When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs<br/>
            Abhirama Subramanyam Penamakuri*, Navlika Singh*, Piyush Arora*, <b>Anand Mishra</b> (*: Equal contribution)
          <br/>
                <i> <b>EMNLP 2025.</b><span class="blinking">(NEW)</span> <br/>
           </li>
          [<a target="_blank" rel="nofollow" href="https://arxiv.org/abs/2509.16633">Paper</a>]
          [<a target="_blank" rel="nofollow" href="https://github.com/vl2g/MPA">Code</a>]
                     <br/> <br/>

           <li>  Aligning Moments in Time using Video Queries<br/>
             Yogesh Kumar*, Uday Agarwal*, Manish Gupta, <b>Anand Mishra</b> (*: Equal contribution)
          <br/>
                <i> <b>ICCV 2025.</b><span class="blinking">(NEW)</span> <br/>
           </li>
          [<a target="_blank" rel="nofollow" href="https://github.com/vl2g/MATR/blob/main/assets/kumar_iccv25.pdf">Paper</a>]
          [<a target="_blank" rel="nofollow" href="https://github.com/vl2g/MATR.git">Code</a>]
                     <br/> <br/>
          
         
  

             <li> AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval<br/>
             Suyash Maniyar*, Vishvesh Trivedi*, Ajoy Mondal, <b>Anand Mishra</b>, C.V. Jawahar (*: Equal contribution)
          <br/>
                <i> <b>ICDAR 2025.</b><span class="blinking">(NEW)</span> <br/>
           </li>
          [<a target="_blank" rel="nofollow" href="https://www.arxiv.org/pdf/2506.23605">Paper</a>]
          [<a target="_blank" rel="nofollow" href="https://synslidegen.github.io/">Project Page</a>]
          [<a target="_blank" rel="nofollow" href="https://github.com/synslidegen/synslidegen_pipeline">Code</a>]
                     <br/> <br/>
          
           <li> PatentLMM: Large Multimodal Model for Generating Descriptions for Patent Figures,<br/>
             Shreya Shukla*, Nakul Sharma*, Manish Gupta, <b>Anand Mishra</b> (*: Equal contribution). 
          <br/>
                <i> <b>AAAI 2025.</b><span class="blinking">(NEW)</span> <br/>
           </li>
          [<a target="_blank" rel="nofollow" href="./files/Shreya-AAAI25.pdf">Paper</a>]
          [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/PatentLMM/">Project Page</a>]
          [<a target="_blank" rel="nofollow" href="https://github.com/vl2g/PatentLMM">Code</a>]
          [<a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=ambAXb65cVY">Short Talk</a>]
           <br/> <br/>
          
          <li> Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant,<br/>
           Abhirama Subramanyam Penamakuri, <b>Anand Mishra</b>. 
          <br/>
                <i> <b>EMNLP 2024.</b> <br/>
           </li>
          [<a target="_blank" rel="nofollow" href="./files/Abhirama-EMNLP24.pdf">Paper</a>]
          [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/LMM4Text-KVQA/">Project Page</a>]
          [<a target="_blank" rel="nofollow" href="https://github.com/vl2g/KaLMA">Code</a>]
          [<a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=ys7NixKot8M">Short Talk</a>]
    
           <br/> <br/>

      
          <li> Show Me the World in My Language: Establishing the First Baseline for Scene-Text to Scene-Text Translation,<br/>
           Shreyas Vaidya<sup>*</sup>, Arvind Kumar Sharma<sup>*</sup>, Prajwal Gatti, <b>Anand Mishra</b>. (*: equal contribution)
          <br/>
                <i> <b>ICPR 2024.</b> <br/>
           </li>
          [<a target="_blank" rel="nofollow" href="./files/Vaidya-ICPR24.pdf">Paper</a>] 
          [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/visTrans/">Project Page</a>]
          [<a target="_blank" rel="nofollow" href="https://github.com/Bhashini-IITJ/visualTranslation/">Code</a>]
          [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/visTrans/resources/ICPR2024-poster.pdf">Poster</a>] 
          [<a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=xvetn4t3tEI">Short Talk</a>]
          <br/> <br/>
         
        <li> QDETRv: Query-Guided DETR for One-Shot Object Localization in Videos,<br/>
            Yogesh Kumar, Saswat Mallick, <b>Anand Mishra</b>, Sowmya Rasipuram, Anutosh Maitra, Roshni Ramnani
          <br/>
                <i> <b>AAAI 2024.</b> <br/>
           </li>
          [<a target="_blank" rel="nofollow" href="./files/Kumar-AAAI24.pdf">Paper</a>] 
           <br/> <br/> 

            <li> Composite Sketch+Text Queries for Retrieving Objects with Elusive Names and Complex Interactions, <br/>
           Prajwal Gatti, Kshitij Parikh, Dhriti Paul, Manish Gupta, <b>Anand Mishra</b>.
          <br/>
                <i> <b>AAAI 2024.</b> <br/>
           </li>
          [<a target="_blank" rel="nofollow" href="./files/Gatti-AAAI24.pdf">Paper</a>]
          [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/cstbir/">Project Page</a>]
          [<a target="_blank" rel="nofollow" href="https://drive.google.com/drive/folders/1UgAZc5rtbO0MQ37WHS4hGQhXlqMPT6Lg">CSTBIR Dataset</a>]
          [<a target="_blank" rel="nofollow" href="https://github.com/vl2g/CSTBIR">Code</a>]
          
           <br/>  <br/> 
            
          <li> Query-guided Attention in Vision Transformers for Localizing Objects Using a Single Sketch, <br/>
            Aditay Tripathi, <b>Anand Mishra</b>, Anirban Chakraborty
          <br/>
                <i> <b>WACV 2024.</b><span class="blinking">(NEW)</span> <br/>
             [<a target="_blank" rel="nofollow" href="./files/Tripathi-WACV24.pdf">Paper</a>][<a target="_blank" rel="nofollow" href="https://vcl-iisc.github.io/locformer/">Project Page</a>][<a target="_blank" rel="nofollow" href="https://github.com/vcl-iisc/locformer-SGOL">Code</a>] 
          </li>
           <br/>    

           <li> Semantic Labels-Aware Transformer Model for Searching over a Large Collection of Lecture-Slides,   <br />
            K.V. Jobin, <b>Anand Mishra</b>, C. V. Jawahar
          <br/>
                <i> <b>WACV 2024 (Oral).</b> <span class="blinking">(NEW: Best Paper Award Finalist)</span> <br />
            [<a target="_blank" rel="nofollow" href="./files/Jobin-WACV24.pdf">Paper</a>][<a target="_blank" rel="nofollow" href="https://jobinkv.github.io/lecsd/">Project Page</a>]
            [<a target="_blank" rel="nofollow" href="https://github.com/jobinkv/LecSD">LecSD Dataset</a>][<a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=2o2fJyXUTDk">Short Talk</a>]
           </li>  
           <br/>
                  
          <li> Answer Mining from a Pool of Images: Towards Retrieval-Based Visual Question Answering, <br />
            Abhirama Subramanyam Penamakuri, Manish Gupta, Mithun Das Gupta, <b>Anand Mishra</b>
          <br/>
                <i> <b>IJCAI 2023. </b> <br />
              [<a target="_blank" rel="nofollow" href="https://www.ijcai.org/proceedings/2023/0146.pdf">Paper</a>][<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/retvqa/">Project Page</a>][<a target="_blank" rel="nofollow" href="https://github.com/Abhiram4572/mi_bart">Code</a>]

          </li>
           
                <br/>  
           <li> Towards Making Flowchart Images Machine Interpretable,  <br />
           Shreya Shukla, Prajwal Gatti, Yogesh Kumar, Vikash Yadav, <b>Anand Mishra</b>
          <br/>
                <i> <b>ICDAR 2023. </b> <br />
                  [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/floco/docs/FLOCO-ICDAR2023.pdf">Paper</a>][<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/floco/">Project Page</a>][<a target="_blank" rel="nofollow" href="https://github.com/vl2g/floco">Code</a>]
          </li>
          
                <br/>     
                 
               
                  
           <li> Few-Shot Referring Relationships in Videos, 
               <br />
            Yogesh Kumar, <b>Anand Mishra</b>
          <br/>
                <i> <b>CVPR 2023. </b> <br />
                  [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/refRelations/docs/paper.pdf">Paper</a>][<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/refRelations/">Project Page</a>][<a target="_blank" rel="nofollow" href="https://github.com/vl2g/RefRelations">Code</a>]
                  </li>
            <br/> 
          
           <li> Grounding Scene Graphs on Natural Images via Visio-Lingual Message Passing
              <br />
            Aditay Tripathi, <b>Anand Mishra</b>, Anirban Chakraborty,
<br/>
                <i> <b>WACV 2023. </b> <br />
                [<a target="_blank" rel="nofollow" href="https://arxiv.org/pdf/2211.01969.pdf">Paper</a>][<a target="_blank" rel="nofollow" href="https://iiscaditaytripathi.github.io/sgl/">Project Page</a>][<a target="_blank" rel="nofollow" href="https://github.com/IISCAditayTripathi/Scene-graph-localization">Code</a>] 
        </li>
                <br/> 
            
          
          <li> VISTOT: Vision-Augmented Table-to-Text Generation,
               <br />
            Prajwal Gatti, <b>Anand Mishra</b>, Manish Gupta, Mithun Das Gupta,<br/>
                <i> <b>EMNLP 2022. </b> <br />
                [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/vistot/docs/VISTOT-EMNLP2022.pdf">Paper</a>][<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/vistot/">Project Page</a>][<a target="_blank" rel="nofollow" href="https://github.com/vl2g/vistot">Code</a>] 
        </li>
                <br/>
            
      
          
          
          <li> COFAR: Commonsense and Factual Reasoning in Image Search
               <br />
            Prajwal Gatti, Abhirama Subramanyam Penamakuri, Revant Teotia, <b>Anand Mishra</b>, Shubhashis Sengupta, Roshni Ramnani<br/>
                <i> <b>AACL-IJCNLP 2022. </b> <br />
                [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/cofar/docs/COFAR-AACL2022.pdf">Paper</a>][<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/cofar/">Project Page</a>][<a target="_blank" rel="nofollow" href="https://github.com/vl2g/cofar">Code</a>] 
        </li>
                <br/>
          
          
          
          <li> Few-shot Visual Relationship Co-localization 
                <br />
                 Revant Teotia<sup>*</sup>, Vaibhav Mishra<sup>*</sup>, Mayank Maheshwari<sup>*</sup>, <b>Anand Mishra</b>,<br/>
                <i> <b>ICCV 2021. </b> <br />
                [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/vrc/docs/VRC-ICCV2021.pdf">Paper</a>][<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/vrc/">Project Page</a>][<a target="_blank" rel="nofollow" href="https://github.com/vl2g/VRC.git">Code</a>] 
                  (*: equal contribution)
                  </li>
                <br/>
          
                <li> Look, Read and Ask: Learning to Ask Questions by Reading Text in Images
                ,<br />
                 Soumya Jahagirdar, Shankar Gangisetty, <b>Anand Mishra</b>,<br/>
                <i> <b>ICDAR 2021 (Oral). </b> <br />
                [<a target="_blank" rel="nofollow" href="./files/JahagirdarICDAR2021.pdf">Paper</a>]           
                  </li>
                <br/>
          
                <li>
                Sketch-Guided Object Localization in Natural Images,<br />
                Aditay Tripathi, Rajath R. Dani, <b>Anand Mishra</b>, Anirban Chakraborty<br/>
                <i> <b>ECCV 2020 (Spotlight Presentation)</b>. <br />
                [<a target="_blank" rel="nofollow" href="./files/TripathiECCV2020.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/ECCV2020.bib'>bibtex</a>]
                [<a href="http://visual-computing.in/sketch-guided-object-localization/">Project page</a>][<a href="https://github.com/IISCAditayTripathi/SketchGuidedLocalization">Code</a>]
                [<a href="https://www.youtube.com/watch?v=vI0NEVytLfU">Know the paper in 90 seconds</a>] [<a href="https://www.youtube.com/watch?v=5gBDPbbvssk">Know the paper in ten minutes</a>]
                </li>
                <br/>

                <li>
                From Strings to Things: Knowledge-enabled VQA model that can read and reason,<br />
                Ajeet Kumar Singh, <b>Anand Mishra</b>, Shashank Shekhar, and Anirban Chakraborty<br/>
                <i> <b>ICCV 2019 (oral)</b>. <br />
                [<a target="_blank" rel="nofollow" href="https://textkvqa.github.io/docs/textKVQA_ICCV2019.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/ICCV2019.bib'>bibtex</a>]
                [<a href="https://textkvqa.github.io">Project page</a>]
                </li>
                <br/>

                <li>
                OCR-VQA: Visual Question Answering by Reading Text in Images <br />
                <b>Anand Mishra</b>, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty<br/>
                <i> <b>ICDAR 2019</b>. <br />
                [<a target="_blank" rel="nofollow" href="./files/mishra-OCR-VQA.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/ICDAR2019.bib'>bibtex</a>]
                [<a href="https://ocr-vqa.github.io">Project page</a>]
                </li><br/>

                <li>
                KVQA: Knowledge-aware Visual Question Answering <br />
                Sanket Shah*, <b>Anand Mishra*</b>, Naganand Yadati and Partha Pratim Talukdar<br/> (*: equal contribution)
                <i> <b>AAAI 2019</b>. (acceptance rate: 16.1%) <br />
                [<a target="_blank" rel="nofollow" href="http://dosa.cds.iisc.ac.in/kvqa/KVQA-AAAI2019.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/aaai19.bib'>bibtex</a>]
                [<a href="http://malllabiisc.github.io/resources/kvqa/">Project page</a>]
                </li><br/>

                <li>
                Deep Embedding using Bayesian Risk Minimization with Application to Sketch Recognition <br />
                <b>Anand Mishra</b>,and Ajeet Kumar Singh<br/>
                <i> <b>ACCV, 2018</b>. (acceptance rate: 28%) <br />
                [<a target="_blank" rel="nofollow" href="https://arxiv.org/abs/1812.02466">Paper (arXiv)</a>] [<a target="_blank" rel="nofollow"href='./files/accv18.bib'>bibtex</a>]
                </li><br/>

                <li>
                IIIT-CFW: A Benchmark database of Cartoon Faces in the Wild<br />
                Ashutosh Mishra, Shyam N. Roy, <b>Anand Mishra</b>,and C. V. Jawahar<br/>
 <i> <b>ECCVW, 2016</b>. (Oral) <br />
                [<a target="_blank" rel="nofollow" href="http://cvit.iiit.ac.in/images/ConferencePapers/2016/Mishra-ECCVW2016.pdf">PDF</a>] [<a target="_blank" rel="nofollow" href='./files/eccvw18.bib'>bibtex</a>][<a href="https://cvit.iiit.ac.in/research/projects/cvit-projects/cartoonfaces"> IIIT-CFW dataset</a>]
                </li><br/>

                <li>
                A Simple and Effective method for Script Identification in the Wild<br />
                Ajeet Kumar Singh, <b>Anand Mishra</b>, Pranav Dabaral and C. V. Jawahar<br/>
                <i> <b>DAS, 2016</b>. <br />
                [<a target="_blank" rel="nofollow" href="https://researchweb.iiit.ac.in/~anand.mishra/Home_files/singhDAS2016.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/das16.bib'>bibtex</a>]
                </li><br/>

                <li>
                Scene Text Recognition and Retrieval for Large Lexicons<br />
                Udit Roy, <b>Anand Mishra</b>, Karteek Alhari and C. V. Jawahar<br/>
                <i> <b>ACCV 2014</b>. <br />
                [<a target="_blank" rel="nofollow" href="https://hal.inria.fr/hal-01088739/document">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/accv14.bib'>bibtex</a>]
                </li><br/>

                <li>
                Image Retrieval using Textual Cues<br />
                <b>Anand Mishra</b>, Karteek Alhari and C. V. Jawahar<br/>
                <i> <b>ICCV, 2013</b>. <br />
                [<a target="_blank" rel="nofollow" href="./files/mishraICCV13.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/iccv13.bib'>bibtex</a>]
                </li><br/>

                <li>
                Whole is Greater than Sum of Parts: Recognizing Scene Text Words<br />
                Vibhor Goel, <b>Anand Mishra</b>, Karteek Alhari and C. V. Jawahar<br/>
                <i> <b>ICDAR, 2013</b>. <br />
                [<a target="_blank" rel="nofollow" href="https://researchweb.iiit.ac.in/~anand.mishra/Home_files/goelICDAR13.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/icdar13_g.bib'>bibtex</a>]
                </li><br/>

                <li>
                Scene Text Recognition using Higher Order Language Priors<br />
                <b>Anand Mishra</b>, Karteek Alhari and C. V. Jawahar<br/>
                <i> <b>BMVC 2012</b>. (Oral) <br />
                [<a target="_blank" rel="nofollow" href="https://researchweb.iiit.ac.in/~anand.mishra/Home_files/mishraBMVC12.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/bmvc12.bib'>bibtex</a>]
                        [<a href="http://cvit.iiit.ac.in/projects/SceneTextUnderstanding/IIIT5K.html"> IIIT-5K Word dataset</a>]
                </li><br/>


                <li>
                Top-down and Bottom-up cues for Scene Text Recognition<br />
                <b>Anand Mishra</b>, Karteek Alhari and C. V. Jawahar<br/>
                <i> <b>CVPR 2012</b>. <br />
                [<a target="_blank" rel="nofollow" href="https://researchweb.iiit.ac.in/~anand.mishra/Home_files/mishraCVPR12.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/cvpr12.bib'>bibtex</a>]
                </li><br/>

                <li>
                An MRF model for Binarization of Natural Scene Text<br />
                <b>Anand Mishra</b>, Karteek Alhari and C. V. Jawahar<br/>
  <i> <b>ICDAR 2011</b>. (Oral) <br />
                [<a target="_blank" rel="nofollow" href="https://researchweb.iiit.ac.in/~anand.mishra/Home_files/mishraICDAR11.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/bmvc11.bib'>bibtex</a>]
                </li>

                <br/>
                </ul>
        </td>
      </tr>
      </tbody></table>
</div>


<div id="teaching">
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="100%" valign="middle">
        <heading>Teaching</heading> <br>
          <h3> At IIT Jodhpur </h3>
          <li> CSL7130: Mathematical Foundations for Computer Science (Monsoon’25)</li>
          <li> CSL2050: Pattern Recognition and Machine Learning (Spring’24/25)</li>
          <li> CSL7670: Fundamentals of Machine Learning (Monsoon’23/24)</li>
          <li> CSL7360: Computer Vision (Spring’23)</li>
          <li> CSL2040: Maths for Computing (Monsoon’22/21/AY 20-21–Tri-3) </li>
          <li> CSL7410: Graph Theory and Application (AY 20-21–Tri-1, Spring’22)</li>
          <li> CS222: Theory of Computation (Spring’20)</li>
          <li> CS212: Object-oriented Design and Analysis (Monsoon’19)</li>
 
           <h3> At IISc Bangalore</h3>
          <li> UE101: Algorithms and Programming (Monsoon'18)</li>
              - Co-taught at Indian Institute of Science with Dr. Sathish Govindrajan and Dr. Viraj Kumar. 
                
            <h3> At IIIT Hyderabad (during PhD)</h3>
          <li> Computer Problem Solving (Monsoon'16) </li>
               -- Introductory course for M.Tech. Bioinformatics
          
          <h3> At IIIT Sri City (during PhD)</h3>
          <li> Computer Architecture (Spring'15)</li>
           -- Co-taught at IIIT Sricity as a visiting instructor with Dr. Suresh Purini and Prof. Govindrajulu</li>
                
        <li> Operating Systems (Monsoon'14) </li>
        -- Co-taught at IIIT Sricity as a visiting instructor with Dr. Suresh Purini </li>
                

    </td>
      </tr>
      </tbody></table>
</div>

 <div id="openPos">
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="100%" valign="middle">
        <heading>Open Positions</heading> <br>
         <div class="sticky-note">
        <p><b>We do not offer any short-term (summer/winter) internship positions, except occasional special calls for specific project needs. 
        I apologize for not being able to respond to individual emails regarding these positions.</b>
       </div>
          
    <p><b>For Non-IITJ students:</b> below are the current open positions:</p>
    <ul>
        <li><b>PhD/MTech-PhD Position:</b> We have open positions for Full-time PhD and MTech-PhD currently. If you are interested in pursuing PhD or MTech-PhD, please consider applying through the institute's official route:
            <a href="https://iitj.ac.in/academics/misc.php?id=advertisements">Link</a>.
        </li>
        <li><b>Research Assistant/Research Engineers/Pre Doc Position:</b>We hire highly motivated engineering graduates, those <b>who graduated or graduating this semester</b> BTech/BE, preferably in CS/EE/AI for "Full-Time (in-person)" Research Engineer or Research Assistant positions.
       This is a rolling call. Exceptional academic credentials, sound machine learning and deep learning knowledge, good programming skills, and passion for doing world-class research (and development) are essential for these positions. If you are eligible and interested, please consider applying <a href="https://forms.gle/8WJttYsryS3A8vKUA"> HERE</a>.
       Next cut-off date: December 15, 2024. <span class="blinking">(NEW)</span>
    </li>   
        </td>
      </tr>
      </tbody></table>
</div>
   

    
